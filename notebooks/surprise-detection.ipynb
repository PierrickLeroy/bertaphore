{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PierrickLeroy/bertaphore/blob/master/notebooks/surprise-detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAZPBRpI9195"
      },
      "outputs": [],
      "source": [
        "# on colab run this cell to install the required packages\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5z_YBPtKCms"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DistilBertForMaskedLM\n",
        "from transformers import DistilBertTokenizer\n",
        "from transformers import CamembertTokenizerFast, CamembertForMaskedLM\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ph9u6HAKCmt"
      },
      "outputs": [],
      "source": [
        "# en\n",
        "model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# fr\n",
        "# model = CamembertForMaskedLM.from_pretrained(\"camembert-base\")\n",
        "# tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AghmJAO4KCmt"
      },
      "outputs": [],
      "source": [
        "def create_maskedSentences(text):\n",
        "    masked_sentences = []\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    n_tokens = inputs[\"input_ids\"].shape[1]\n",
        "    for i in range(1, n_tokens-1):\n",
        "        s = tokenizer(text, return_tensors=\"pt\")\n",
        "        s[\"labels\"] = s[\"input_ids\"].detach().clone()\n",
        "        t = torch.zeros(n_tokens, dtype=torch.bool)\n",
        "        t[i] = True\n",
        "        s[\"input_ids\"][0,i] = 103  # MASK token\n",
        "        masked_sentences.append({\"inputs\": s, \"mask\": t, \"id_masked\": i})\n",
        "    return masked_sentences\n",
        "\n",
        "def print_inputs(inputs):\n",
        "    return \" \".join(list(map(tokenizer.convert_ids_to_tokens, inputs[\"input_ids\"]))[0])\n",
        "\n",
        "def parse_sentences(sentences):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        sentences (list): list of dict containing keys :\n",
        "            - 'inputs' : transformers.tokenization_utils_base.BatchEncoding with 'input_ids', 'attention_mask' and 'labels'. One of the tokens in input_ids is a MASK token (103)\n",
        "            - 'id_masked' : index of the masked word\n",
        "    \"\"\"\n",
        "    for sentence in sentences:\n",
        "        inputs = sentence[\"inputs\"]\n",
        "        loss, logits = model(**inputs).values()\n",
        "        score_surprise = ((logits[:, sentence[\"id_masked\"], :].argsort(descending=True) == sentence[\"inputs\"][\"labels\"][0,sentence[\"id_masked\"]]).flatten()).nonzero()[0,0].item()\n",
        "        sentence[\"score_surprise\"] = score_surprise\n",
        "\n",
        "def plot_barSurprise(sentences):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        sentences (list): list of dict containing keys :\n",
        "            - 'inputs' : transformers.tokenization_utils_base.BatchEncoding with 'input_ids', 'attention_mask' and 'labels'. One of the tokens in input_ids is a MASK token (103)\n",
        "            - 'id_masked' : index of the masked word\n",
        "            - 'score_surprise' (float): high value = big surprise\n",
        "    \"\"\"\n",
        "    scores = [x[\"score_surprise\"] for x in sentences]\n",
        "    words = list(map(tokenizer.convert_ids_to_tokens, sentences[0][\"inputs\"][\"labels\"]))[0][1:-1]\n",
        "    plt.bar(words, scores)\n",
        "    plt.gca().margins(x=0)\n",
        "    plt.gcf().canvas.draw()\n",
        "    tl = plt.gca().get_xticklabels()\n",
        "    maxsize = max(t.get_window_extent().width for t in tl)\n",
        "    m = 1 # inch margin\n",
        "    s = maxsize/plt.gcf().dpi*len(words)+2*m\n",
        "    margin = m/plt.gcf().get_size_inches()[0]\n",
        "\n",
        "    plt.gcf().subplots_adjust(left=margin, right=1.-margin)\n",
        "    plt.gcf().set_size_inches(s, plt.gcf().get_size_inches()[1])\n",
        "\n",
        "def highlight_text(sentences, lamb = 0.01):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        sentences (_type_): _description_\n",
        "        lamb (float, optional): _description_. Defaults to 0.01.\n",
        "    \"\"\"\n",
        "    scores = [x[\"score_surprise\"] for x in sentences]\n",
        "    words = list(map(tokenizer.convert_ids_to_tokens, sentences[0][\"inputs\"][\"labels\"]))[0][1:-1]\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches((10,6))\n",
        "    plt.axis('off')\n",
        "    offset_y = 0.1\n",
        "    offset_x = 0.01\n",
        "    for i in range(len(words)):\n",
        "        t = plt.text(offset_x + (i%10)/10,1-offset_y-(i//10)/10,words[i])\n",
        "        t.set_bbox(dict(facecolor='red', alpha=1-np.exp(-scores[i]*lamb)))\n",
        "        plt.text(offset_x + 0.01 + (i%10)/10,1-offset_y-0.045-(i//10)/10,scores[i], fontdict={\"fontsize\":7})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3lS9AyDKCmu"
      },
      "outputs": [],
      "source": [
        "# en\n",
        "text_en1 = \"Peter is brave and never gives up. He has the heart of a lion.\"\n",
        "text_en2 = \"Since I started this new job I am always looking forward to meet new people. I am open to all opportunities. I am always searching for my next prey\"\n",
        "text_en3 = \"My lawyer has been my trusted ally, guiding me through the intricate labyrinth of the legal system.\"\n",
        "text_en4 = \"My lawyer has been my trusted ally, guiding me through the intricate labyrinth of the legal system. He is a shark.\"\n",
        "text_en5 = \"My lawyer has been my trusted ally, guiding me through the intricate labyrinth of the legal system. He is searching for his next prey.\"\n",
        "text_en6 = \"My lawyer has been my trusted ally, guiding me through the intricate labyrinth of the legal system. He is a shark searching for his next prey.\"\n",
        "\n",
        "\n",
        "# fr\n",
        "text_baudelaire_soleil = \"Les soleils mouillés de ces ciels brouillés pour mon esprit ont les charmes si mystérieux de tes traîtres yeux, brillant à travers leurs larmes.\"\n",
        "text_baudelaire_ambre = \"Là, tout n’est qu’ordre et beauté, luxe, calme et volupté. Des meubles luisants, polis par les ans, décoreraient notre chambre ; les plus rares fleurs mêlant leurs odeurs aux vagues senteurs de l’ambre, les riches plafonds, les miroirs profonds, la splendeur orientale, tout y parlerait à l’âme en secret sa douce langue natale.\"\n",
        "text_baudelaire_vaisseaux = \"Vois sur ces canaux dormir ces vaisseaux dont l’humeur est vagabonde ; c’est pour assouvir ton moindre désir qu’ils viennent du bout du monde.\"\n",
        "text_bebe = \"Après avoir donné le biberon à votre bébé, le démonter et le laver\"\n",
        "text_rimbaud_tempete = \"La tempête a béni mes éveils maritimes.\"\n",
        "text_rimbaud_mer = \"Et dès lors, je me suis baigné dans le poème de la Mer.\"\n",
        "\n",
        "text = text_en2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATGyyc0pKCmu"
      },
      "outputs": [],
      "source": [
        "masked_sentences = create_maskedSentences(text)\n",
        "# masked_sentences[:2]\n",
        "parse_sentences(masked_sentences)\n",
        "# masked_sentences[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj1mQjMtUy35"
      },
      "outputs": [],
      "source": [
        "highlight_text(masked_sentences, lamb=0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uinMkRP3KCmu"
      },
      "outputs": [],
      "source": [
        "plot_barSurprise(masked_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTBp1tW_KCmv"
      },
      "source": [
        "# <>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "metaphore",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}