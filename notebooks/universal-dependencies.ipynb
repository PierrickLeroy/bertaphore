{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "root_folder_name = \"bertaphore\"\n",
    "p = os.getcwd()\n",
    "while os.path.basename(p) != root_folder_name:\n",
    "    p = os.path.dirname(p)\n",
    "sys.path.insert(0, p)\n",
    "\n",
    "import torch\n",
    "import datetime\n",
    "from conllu import parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from modules import metric\n",
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "from bertviz import model_view, head_view\n",
    "from scipy.linalg import toeplitz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "\n",
    "model_name = \"microsoft/xtremedistil-l12-h384-uncased\"  # Find popular HuggingFace models here: https://huggingface.co/models\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)  # Configure model to return attention values\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burdenizer = metric.AttentionalBurden(model, tokenizer\n",
    "                                      , prune_cls_sep=True\n",
    "                                      , normalize_attention=True\n",
    "                                      )\n",
    "burdenizer.compute_burden(\"Alice is eating pizza. I like Bob.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDERING_DEPREL_UNIVERSAL = ['punct', 'case', 'nsubj', 'det', 'nmod', 'root', 'obl', 'obj', 'amod',\n",
    "       'advmod', 'conj', 'compound', 'aux', 'cc', 'mark', 'cop', 'acl',\n",
    "       'advcl', 'xcomp', 'flat', 'nummod', 'ccomp', 'appos', 'parataxis',\n",
    "       'discourse', 'ref', 'iobj', 'fixed', 'expl', 'csubj', 'dep', 'list',\n",
    "       'reparandum', 'vocative', 'dislocated', 'goeswith', 'orphan']\n",
    "\n",
    "datasets_ud_english = [\"GUM\", \"EWT\", \"LinES\", \"ParTUT\"\n",
    "                     #   , \"Atis\"\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence_dependencies(data):\n",
    "    l_head, l_deprel = [], []\n",
    "    for sentence_id, sentence in enumerate(data):\n",
    "        for token_id, token in enumerate(sentence):\n",
    "            id_ud = token[\"id\"]\n",
    "            if isinstance(id_ud, tuple) and len(id_ud)==3 and id_ud[1]==\"-\":\n",
    "                continue\n",
    "            l_head.append((sentence_id, token_id+1, id_ud, token[\"form\"], token[\"deprel\"], token[\"head\"]))\n",
    "            deps = token[\"deps\"]\n",
    "            if isinstance(deps, list):\n",
    "                for dep in deps:\n",
    "                    l_deprel.append((sentence_id, token_id+1, id_ud, token[\"form\"],)+dep)\n",
    "    return l_head, l_deprel\n",
    "\n",
    "def build_dfUD(l_head, l_deprel):\n",
    "    df_head = pd.DataFrame(l_head, columns=[\"sentence_id\", \"token_id\", \"id\", \"form\", \"deprel\", \"head\"])\n",
    "    df_deprel = pd.DataFrame(l_deprel, columns=[\"sentence_id\", \"token_id\", \"id\", \"form\", \"deprel\", \"head\"])\n",
    "    assert len(df_deprel)==0 or (df_head[[\"sentence_id\", \"token_id\", \"form\"]].drop_duplicates().reset_index(drop=True)==df_deprel[[\"sentence_id\", \"token_id\", \"form\"]].drop_duplicates().reset_index(drop=True)).all(axis=None)\n",
    "    df_head = df_head.dropna(subset=[\"head\"])\n",
    "    assert not df_head.isna().any().any() and not df_deprel.isna().any().any()\n",
    "\n",
    "    # deprel format\n",
    "    df_deprel = df_deprel.merge(df_deprel[[\"sentence_id\", \"id\", \"token_id\"]].drop_duplicates().rename(columns={\"token_id\":\"token_head\"})\n",
    "                ,how=\"left\"\n",
    "                ,left_on=[\"sentence_id\", \"head\"], right_on=[\"sentence_id\", \"id\"]\n",
    "                ,suffixes=(\"\", \"_head\")).drop(columns=[\"id_head\"])\n",
    "    df_deprel[\"deprel_universal\"] = df_deprel[\"deprel\"].apply(lambda u: u.split(\":\")[0])\n",
    "    # df_deprel[\"token_head\"] = df_deprel.apply(lambda u: u[\"token_head\"] if not(pd.isna(u[\"token_head\"])) else u[\"head\"], axis=1).astype(int)\n",
    "    df_deprel.loc[df_deprel[\"deprel\"]==\"root\", \"token_head\"] = 0\n",
    "    df_deprel[\"token_head\"] = df_deprel[\"token_head\"].astype(int)\n",
    "    \n",
    "    # head format\n",
    "    df_head = df_head.merge(df_head[[\"sentence_id\", \"id\", \"token_id\"]].drop_duplicates().rename(columns={\"token_id\":\"token_head\"})\n",
    "                ,how=\"left\"\n",
    "                ,left_on=[\"sentence_id\", \"head\"], right_on=[\"sentence_id\", \"id\"]\n",
    "                ,suffixes=(\"\", \"_head\")).drop(columns=[\"id_head\"])\n",
    "    df_head[\"deprel_universal\"] = df_head[\"deprel\"].apply(lambda u: u.split(\":\")[0])\n",
    "    # df_head[\"token_head\"] = df_head.apply(lambda u: u[\"token_head\"] if not(pd.isna(u[\"token_head\"])) else u[\"head\"], axis=1).astype(int)\n",
    "    df_head.loc[df_head[\"deprel\"]==\"root\", \"token_head\"] = 0\n",
    "    df_head[\"token_head\"] = df_head[\"token_head\"].astype(int)\n",
    "    \n",
    "    df_ud = pd.concat([df_deprel[[\"sentence_id\", \"form\", \"token_id\", \"token_head\", \"deprel_universal\"]]\n",
    "           ,df_head[[\"sentence_id\", \"form\", \"token_id\", \"token_head\", \"deprel_universal\"]]]).drop_duplicates()\n",
    "    df_ud[\"length_relation\"] = df_ud[\"token_head\"] - df_ud[\"token_id\"]\n",
    "    df_ud[\"abs_length_relation\"] = df_ud[\"length_relation\"].abs()\n",
    "    return df_ud\n",
    "\n",
    "def process_data(data):\n",
    "    l_head, l_deprel = parse_sentence_dependencies(data)\n",
    "    df_ud = build_dfUD(l_head, l_deprel)\n",
    "    return df_ud\n",
    "\n",
    "def calculate_attentionalScores(data, verbose=False):\n",
    "    d = {\"score_attentional\":[], \"length_attentional\":[]}\n",
    "    for i, sentence in enumerate(data):\n",
    "        if i % 100 == 0 and verbose:\n",
    "            print(f\"Processing sentence {i}...\")\n",
    "        text = sentence.metadata[\"text\"]\n",
    "        burden = burdenizer.compute_burden(text)\n",
    "        d[\"score_attentional\"].append(burden)\n",
    "        d[\"length_attentional\"].append(len(burdenizer.tokens))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df = []\n",
    "l_df_attentional = []\n",
    "for dataset in datasets_ud_english:\n",
    "    print(f\"Processing {dataset}...\")\n",
    "    data_train = parse(open(f\"/home/pierrick/datasets/Universal Dependencies 2.14/ud-treebanks-v2.14/UD_English-{dataset}/en_{dataset.lower()}-ud-train.conllu\", \"r\").read())\n",
    "    data_test = parse(open(f\"/home/pierrick/datasets/Universal Dependencies 2.14/ud-treebanks-v2.14/UD_English-{dataset}/en_{dataset.lower()}-ud-test.conllu\", \"r\").read())\n",
    "    data_dev = parse(open(f\"/home/pierrick/datasets/Universal Dependencies 2.14/ud-treebanks-v2.14/UD_English-{dataset}/en_{dataset.lower()}-ud-dev.conllu\", \"r\").read())\n",
    "    df_train = process_data(data_train)\n",
    "    df_train[\"set\"] = \"train\"\n",
    "    df_test = process_data(data_test)\n",
    "    df_test[\"set\"] = \"test\"\n",
    "    df_dev = process_data(data_dev)\n",
    "    df_dev[\"set\"] = \"dev\"\n",
    "    df = pd.concat([df_train, df_test, df_dev])\n",
    "    df[\"dataset\"] = dataset\n",
    "    l_df.append(df)\n",
    "    \n",
    "    # attentional burden\n",
    "    # for data, set_label in zip([data_train, data_test, data_dev], [\"train\", \"test\", \"dev\"]):\n",
    "    #     print(f\"Processing attentional burden for {dataset} {set_label}...\")\n",
    "    #     d = calculate_attentionalScores(data, verbose=True)\n",
    "    #     df_attentional = pd.DataFrame(d).reset_index().rename(columns={\"index\":\"sentence_id\"})\n",
    "    #     df_attentional[\"dataset\"] = dataset\n",
    "    #     df_attentional[\"set\"] = set_label\n",
    "    #     l_df_attentional.append(df_attentional)\n",
    "\n",
    "df_ud_english = pd.concat(l_df)\n",
    "# df_attentional = pd.concat(l_df_attentional)\n",
    "# df_attentional = df_attentional.merge(df_ud_english.groupby([\"sentence_id\", \"dataset\", \"set\"]).agg({\"token_id\":\"max\"}).reset_index()\n",
    "                    #  , how=\"left\", on=[\"sentence_id\", \"dataset\", \"set\"]).rename(columns={\"token_id\":\"length_linguistics\"})\n",
    "# display(df_ud_english), display(df_attentional)\n",
    "\n",
    "# save\n",
    "now = datetime.datetime.now()\n",
    "ts = now.strftime('%Y%m%d_%H%M')\n",
    "df_ud_english.to_csv(f\"../data/df_ud_english_{ts}.csv\", index=False)\n",
    "# df_attentional.to_csv(f\"../data/df_attentional_{ts}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ud_english = pd.read_csv(\"../data/df_ud_english_20241006_2219.csv\")\n",
    "df_attentional = pd.read_csv(\"../data/df_attentional_20241004_1208.csv\")\n",
    "display(df_ud_english), display(df_attentional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "s = pd.Series(df_ud_english.groupby(\"deprel_universal\").transform(\"size\"), name=\"size\")\n",
    "df = pd.concat([df_ud_english, s], axis=1)\n",
    "ax = sns.boxplot(data=df, y=\"deprel_universal\", hue=\"size\", x=\"length_relation\", showfliers=False\n",
    "                 , order=ORDERING_DEPREL_UNIVERSAL)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.title(\"Length of relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "s = pd.Series(df_ud_english.groupby(\"deprel_universal\").transform(\"size\"), name=\"size\")\n",
    "df = pd.concat([df_ud_english, s], axis=1)\n",
    "ax = sns.boxplot(data=df, y=\"deprel_universal\", hue=\"size\", x=\"abs_length_relation\", showfliers=False\n",
    "                 , order=ORDERING_DEPREL_UNIVERSAL)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.title(\"Length of relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 20))\n",
    "\n",
    "s = pd.Series(df_ud_english.groupby(\"deprel_universal\").transform(\"size\"), name=\"size\")\n",
    "df = pd.concat([df_ud_english, s], axis=1)\n",
    "ax = sns.boxplot(data=df, y=\"deprel_universal\", hue=\"dataset\", x=\"length_relation\", showfliers=False\n",
    "                 , order=ORDERING_DEPREL_UNIVERSAL)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.title(\"Length of relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 20))\n",
    "\n",
    "s = pd.Series(df_ud_english.groupby(\"deprel_universal\").transform(\"size\"), name=\"size\")\n",
    "df = pd.concat([df_ud_english, s], axis=1)\n",
    "ax = sns.boxplot(data=df, y=\"deprel_universal\", hue=\"dataset\", x=\"abs_length_relation\", showfliers=False\n",
    "                 , order=ORDERING_DEPREL_UNIVERSAL)\n",
    "ax.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.title(\"Length of relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df_ud_english, x=\"dataset\", order=datasets_ud_english)\n",
    "plt.title(\"Number of relations by dataset\")\n",
    "plt.ylabel(\"Number of relations\")\n",
    "\n",
    "# comment: there is two big datasets (EWT and LinES) and three smaller datasets (GUM, ParTUT, Atis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df_ud_english[[\"sentence_id\", \"dataset\", \"set\"]].drop_duplicates(), x=\"dataset\", hue=\"set\"\n",
    "              ,order=datasets_ud_english)\n",
    "plt.title(\"Number of sentences by dataset and set\")\n",
    "plt.ylabel(\"Number of sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ud_english.groupby([\"dataset\", \"set\", \"sentence_id\"], as_index=False).agg({\"token_id\":\"max\"})\n",
    "sns.boxplot(data=df, x=\"dataset\", y=\"token_id\", hue=\"set\", order=datasets_ud_english, showfliers=False)\n",
    "plt.title(\"Length of sentences\")\n",
    "plt.ylabel(\"Tokens\")\n",
    "\n",
    "# comment: ATIS is not like the others, less variability in the length of sentences and shorter sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    df_ud_english.groupby([\"sentence_id\", \"dataset\", \"set\"]).agg({\"token_id\":\"max\"}).reset_index().groupby([\"dataset\"]).agg({\"token_id\":\"sum\"})\n",
    "    , df_ud_english.groupby([\"deprel_universal\", \"dataset\"], as_index=False).size().set_index([\"dataset\"])\n",
    "    , left_index=True, right_index=True, how=\"inner\")\n",
    "df[\"freq\"] = df[\"size\"]/df[\"token_id\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plt.grid(True)\n",
    "plt.title(\"Frequency of relations by dataset\")\n",
    "plt.xlabel(\"Frequency (relation by token)\")\n",
    "sns.scatterplot(data=df.sort_values(by=\"deprel_universal\", key=lambda col: col.map(dict(zip(ORDERING_DEPREL_UNIVERSAL, range(len(ORDERING_DEPREL_UNIVERSAL)))))), y=\"deprel_universal\", x=\"freq\", hue=\"dataset\")\n",
    "\n",
    "# comment: ATIS is not like the others, often its point is an outlier and has no punct for example (speech dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    df_ud_english.groupby([\"sentence_id\", \"dataset\", \"set\"]).agg({\"token_id\":\"max\"}).reset_index().groupby([\"dataset\"]).agg({\"token_id\":\"sum\"})\n",
    "    , df_ud_english.groupby([\"deprel_universal\", \"dataset\"], as_index=False).agg({\"token_id\":\"count\"\n",
    "                                                                            ,\"abs_length_relation\":\"mean\"}).set_index([\"dataset\"]).rename(columns={\"token_id\":\"size\"})\n",
    "    , left_index=True, right_index=True, how=\"inner\").reset_index()\n",
    "df[\"freq\"] = df[\"size\"]/df[\"token_id\"]\n",
    "\n",
    "px.scatter(df, x=\"freq\", y=\"abs_length_relation\", facet_col=\"dataset\", hover_data=[\"deprel_universal\"], color=\"deprel_universal\"\n",
    "           , category_orders={\"dataset\":datasets_ud_english})\n",
    "\n",
    "# comment: again, ATIS seems different, could do a Wasserstein/Bottelneck distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score attentional, sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_attentional[[\"length_linguistics\", \"length_attentional\"]].melt().groupby([\"variable\", \"value\"], as_index=False).size().rename(columns={\"size\":\"count\",\n",
    "                                                                                                                              \"variable\":\"type\",\n",
    "                                                                                                                             \"value\":\"length\"})\n",
    "fig = px.line(df, x=\"length\", y=\"count\", color=\"type\")                                                                                                                        \n",
    "fig.show()\n",
    "\n",
    "# comment: something is weird for short sentences, better to start at 4 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df_attentional, x=\"length_linguistics\", y=\"length_attentional\", facet_col=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "from holoviews.operation.datashader import datashade, rasterize, shade, dynspread, spread\n",
    "from holoviews.operation.resample import ResampleOperation2D\n",
    "from holoviews.operation import decimate\n",
    "from holoviews.operation.datashader import datashade\n",
    "hv.extension('bokeh','matplotlib', width=100)\n",
    "points = hv.Points(df_attentional[['length_linguistics', 'length_attentional']])\n",
    "# points = decimate(points, dynamic=False, max_samples=3000)\n",
    "spread(rasterize(points, width=400, height=400), px=1, shape='square').relabel(\"Rasterized\")\n",
    "\n",
    "# comment: need to filter out sentences that have a big difference between the two lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify df_attentional and df_ud_english\n",
    "df = df_attentional.loc[(df_attentional[\"length_attentional\"]>=4)\n",
    "                   &(df_attentional[\"length_attentional\"]<=50)]\n",
    "mask = np.minimum((df[\"length_attentional\"]/df[\"length_linguistics\"]).values,(df[\"length_linguistics\"]/df[\"length_attentional\"]).values)>0.75\n",
    "df = df.loc[mask]\n",
    "df_attentional = df.reset_index(drop=True)\n",
    "\n",
    "df_ud_english = df_ud_english.merge(df_attentional.reset_index(),\n",
    "                    how=\"inner\",on=[\"sentence_id\",\"dataset\",\"set\"])\n",
    "\n",
    "display(df_attentional), display(df_ud_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.extension('bokeh','matplotlib', width=100)\n",
    "points = hv.Points(df_attentional[['length_linguistics', 'length_attentional']])\n",
    "# points = decimate(points, dynamic=False, max_samples=3000)\n",
    "spread(rasterize(points, width=400, height=400), px=1, shape='square').relabel(\"Rasterized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_attentional.groupby(\"length_attentional\", as_index=False).agg({\"score_attentional\":\"mean\"})\n",
    "# df[\"max_burden\"] = df[\"length_attentional\"].apply(lambda n: metric.compute_maxAttentionalBurden(burdenizer.w_forward, burdenizer.w_backward, n)).astype(float)\n",
    "# df[\"uniform_burden\"] = df[\"length_attentional\"].apply(lambda n: metric.compute_uniformAttentionalBurden(burdenizer.w_forward, burdenizer.w_backward, n))\n",
    "# df[\"max_burden_equal_vertices\"] = df[\"length_attentional\"].apply(lambda n: metric.compute_maxAttentionalBurdenEqualVertices(burdenizer.w_forward, burdenizer.w_backward, n))\n",
    "df[f\"half_hop_burden\"] = df[\"length_attentional\"].apply(lambda n: metric.compute_kHopAttentionalBurden(burdenizer.w_forward, burdenizer.w_backward, n, int(np.ceil(n/2))))\n",
    "# df[f\"2_hop_burden\"] = df[\"length_attentional\"].apply(lambda n: metric.compute_kHopAttentionalBurden(burdenizer.w_forward, burdenizer.w_backward, n, 2))\n",
    "# df[f\"9_hop_burden\"] = df[\"length_attentional\"].apply(lambda n: metric.compute_kHopAttentionalBurden(burdenizer.w_forward, burdenizer.w_backward, n, 9))\n",
    "df = df.rename(columns={\"score_attentional\":\"norme\"\n",
    "                        ,\"max_burden\":\"max\"\n",
    "                        ,\"uniform_burden\":\"uniform\"})\n",
    "df = df.melt(id_vars=[\"length_attentional\"]).dropna()\n",
    "fig = px.line(df,\n",
    "        x=\"length_attentional\", y=\"value\", color=\"variable\")\n",
    "fig.update_layout(yaxis_range=[0,15]\n",
    "                  ,xaxis_title=\"Longueur du texte\",\n",
    "                yaxis_title=\"Score attentionnel\",\n",
    "                legend_title=\"Légende\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation score attentional score linguistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df_spearman = []\n",
    "for deprel in df_ud_english[\"deprel_universal\"].unique():\n",
    "    df = pd.merge(df_attentional\n",
    "        ,df_ud_english.loc[df_ud_english[\"deprel_universal\"]==deprel].groupby(\"index\").agg({\"abs_length_relation\":\"mean\"}).reset_index().rename(columns={\"abs_length_relation\":\"score_linguistics\"})\n",
    "        ,how=\"inner\", left_index=True, right_on=\"index\")\n",
    "    l_spearmanr = []\n",
    "    for length_attentional in np.sort(df[\"length_attentional\"].unique()):\n",
    "        a = df.loc[df[\"length_attentional\"]==length_attentional, [\"score_attentional\", \"score_linguistics\"]].values\n",
    "        a_attentional = a[:, 0]\n",
    "        a_linguistics = a[:, 1]\n",
    "        spearmanr = scipy.stats.spearmanr(a_attentional, a_linguistics)\n",
    "        l_spearmanr.append((deprel, length_attentional, spearmanr.correlation, spearmanr.pvalue, len(a)))\n",
    "    l_df_spearman.append(pd.DataFrame(l_spearmanr, columns=[\"deprel\", \"length_attentional\", \"correlation\", \"pvalue\", \"size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spearman = pd.concat(l_df_spearman).dropna()\n",
    "df_spearman[\"has_significant_pvalue\"] = df_spearman[\"pvalue\"]<0.05\n",
    "df_spearman = df_spearman.loc[(df_spearman[\"has_significant_pvalue\"])&(df_spearman[\"size\"]>10)&(~df_spearman[\"deprel\"].isin([\"punct\", \"root\"]))]\n",
    "df_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_spearman\n",
    "        , x=\"length_attentional\"\n",
    "        , y=\"correlation\"\n",
    "        , color=\"deprel\"\n",
    "        , hover_data=[\"deprel\", \"pvalue\", \"size\"])\n",
    "fig.show()\n",
    "# fig.write_html(\"../images/universal-dependencies/UD_EWT_spearman_correlation_linguistics_vs_attentional_by_length.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spearman.groupby(\"deprel\").agg({\"correlation\":\"mean\", \"size\":\"mean\", \"deprel\":\"count\"}).rename(columns={\"deprel\":\"count_deprel\"}).sort_values(\"count_deprel\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_deprel_selected = ['nsubj', 'aux', 'advcl', 'conj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_attentional\n",
    "    ,df_ud_english.loc[df_ud_english[\"deprel_universal\"].isin(l_deprel_selected)].groupby(\"index\").agg({\"abs_length_relation\":\"mean\"}).reset_index().rename(columns={\"abs_length_relation\":\"score_linguistics\"})\n",
    "    ,how=\"inner\", left_index=True, right_on=\"index\")\n",
    "l_spearmanr = []\n",
    "for length_attentional in np.sort(df[\"length_attentional\"].unique()):\n",
    "    a = df.loc[df[\"length_attentional\"]==length_attentional, [\"score_attentional\", \"score_linguistics\"]].values\n",
    "    a_attentional = a[:, 0]\n",
    "    a_linguistics = a[:, 1]\n",
    "    spearmanr = scipy.stats.spearmanr(a_attentional, a_linguistics)\n",
    "    l_spearmanr.append((\"combined\", length_attentional, spearmanr.correlation, spearmanr.pvalue, len(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "df = pd.DataFrame(l_spearmanr, columns=[\"deprel\", \"length_attentional\", \"correlation\", \"pvalue\", \"size\"])\n",
    "df[\"significant_pvalue\"] = df[\"pvalue\"]<0.05\n",
    "fig = px.line(df, x=\"length_attentional\", y=\"correlation\", markers=False\n",
    "        ,hover_data=[\"pvalue\", \"size\"])\n",
    "fig.add_trace(go.Scatter(x=df[\"length_attentional\"], y=df[\"correlation\"], mode='markers', marker=dict(color=df[\"significant_pvalue\"].map({True:\"Green\", False:\"Red\"})\n",
    "                                                                                                      , size=df[\"size\"]*0.025)))\n",
    "fig.update_layout(title=f\"Relation used: \"+\", \".join(l_deprel_selected) + \" (red: p-value > 0.05, green: p-value < 0.05, size: number of samples)\")\n",
    "# fig.write_html(\"../images/universal-dependencies/UD_EWT_spearman_correlation_linguistics_vs_attentional_by_length_combined.html\")\n",
    "fig.update_layout(xaxis_title=\"Longueur du texte\",\n",
    "                yaxis_title=\"Correlation\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text1 = \"Joe is running for president. I think I will vote for Donald. He is a good candidate.\"\n",
    "input_text2 = \"Joe is running for president. I think I will vote for him. He is a good candidate.\"\n",
    "t_attention1 = burdenizer._get_attention(input_text1).mean(dim=[0,1], keepdim=True)\n",
    "tokens1 = burdenizer.tokens\n",
    "t_attention2 = burdenizer._get_attention(input_text2).mean(dim=[0,1], keepdim=True)\n",
    "tokens2 = burdenizer.tokens\n",
    "\n",
    "tokens_merged = []\n",
    "for i in range(len(tokens1)):\n",
    "    if tokens1[i] == tokens2[i]:\n",
    "        tokens_merged.append(tokens1[i])\n",
    "    else:\n",
    "        tokens_merged.append(f\"{tokens1[i]}/{tokens2[i]}\")\n",
    "\n",
    "# normalize\n",
    "t_attention1 = t_attention1/t_attention1.abs().sum()\n",
    "t_attention2 = t_attention2/t_attention2.abs().sum()\n",
    "\n",
    "t = torch.concatenate([torch.nn.ReLU()(t_attention1 - t_attention2)\n",
    "                   , torch.nn.ReLU()(t_attention2 - t_attention1)]\n",
    "                  , dim=1)\n",
    "\n",
    "print(f\"Attentional burden 1: {burdenizer.compute_burden(input_text1)}\")\n",
    "print(f\"Attentional burden 2: {burdenizer.compute_burden(input_text2)}\")\n",
    "head_view([t*1500], tokens=tokens_merged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertaphore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
